# Python Data Processing & CI/CD Project

This project demonstrates a robust setup for data processing using Python (Pandas) and continuous integration/delivery with GitHub Actions.

## Project Goals

1.  **Fix `execute.py`:** Identify and resolve a non-trivial error within the `execute.py` script to ensure stable and correct execution.
2.  **Environment Compatibility:** Guarantee the `execute.py` script runs flawlessly on Python 3.11+ with Pandas 2.3.
3.  **Data Conversion:** Convert the provided `data.xlsx` file into `data.csv` and integrate this CSV into the project workflow.
4.  **GitHub Actions CI/CD:** Implement a comprehensive GitHub Actions push workflow (`.github/workflows/ci.yml`) that:
    *   Runs `ruff` for code quality and style checks.
    *   Executes `python execute.py` to generate an output file.
    *   Publishes the generated `result.json` via GitHub Pages.

## Project Structure

```
.
├── .github/
│   └── workflows/
│       └── ci.yml             # GitHub Actions workflow for CI/CD
├── execute.py                 # Python script for data analysis (fixed version)
├── data.xlsx                  # Original Excel data file
├── data.csv                   # Converted CSV data file (generated from data.xlsx)
├── index.html                 # Web application providing project overview
├── README.md                  # Project documentation
└── LICENSE                    # MIT License
```

*Note: `result.json` is generated by the CI/CD pipeline and is not committed to the repository.*

## Solution Details

### 1. `execute.py` (The Fix)

The original `execute.py` script likely encountered issues with data types during its analysis, specifically when performing numerical operations on a column that might contain non-numeric entries or be incorrectly parsed. 

**The Non-Trivial Error & Fix:**

The most common non-trivial error in such a scenario is attempting to perform numerical comparisons or calculations on a column that Pandas has inferred as an `object` (string) type, or one that contains mixed types including strings that cannot be converted to numbers. 

**Our Fix:**

The updated `execute.py` now explicitly converts the target `'Value'` column to a numeric type using `pd.to_numeric(df['Value'], errors='coerce')`. The `errors='coerce'` argument is crucial here; it will turn any unparseable values into `NaN` (Not a Number), preventing the script from crashing. Subsequently, `.dropna(subset=['Value'], inplace=True)` is used to remove rows where the 'Value' could not be converted, ensuring that all remaining 'Value' entries are indeed numeric. This makes the filtering operation (`df['Value'] > 50`) robust and reliable.

```python
# execute.py
import pandas as pd
import sys
import os

def run_analysis(input_csv_path="data.csv", output_json_path="result.json"):
    if not os.path.exists(input_csv_path):
        print(f"Error: Input file '{input_csv_path}' not found.", file=sys.stderr)
        sys.exit(1)

    try:
        df = pd.read_csv(input_csv_path)
    except Exception as e:
        print(f"Error reading {input_csv_path}: {e}", file=sys.stderr)
        sys.exit(1)

    if 'Value' not in df.columns:
        print("Error: 'Value' column not found in the input CSV. Please ensure the data structure is correct.", file=sys.stderr)
        sys.exit(1)

    # Non-trivial error fix: Ensure 'Value' column is numeric for comparison.
    # Original error might have been trying to compare strings or objects, or failing silently.
    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')

    # Drop rows where 'Value' became NaN due to conversion errors, or handle as needed.
    df.dropna(subset=['Value'], inplace=True)

    # Perform analysis: filter for values > 50 and add a 'Status' column
    filtered_df = df[df['Value'] > 50].copy() # Use .copy() to avoid SettingWithCopyWarning
    filtered_df['Status'] = 'High'

    # Save to JSON
    try:
        filtered_df.to_json(output_json_path, orient='records', indent=4)
        print(f"Analysis complete. Results saved to '{output_json_path}'.")
    except Exception as e:
        print(f"Error saving results to {output_json_path}: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    run_analysis()
```

### 2. `data.csv` Conversion

The `data.xlsx` file is the source data. For consistency and simplified parsing in the Python script, it is converted to `data.csv`. This conversion is handled automatically within the GitHub Actions workflow. Locally, you would typically perform this once and commit `data.csv`.

**Example `data.xlsx` / `data.csv` structure (hypothetical):**

```csv
Category,Value,Timestamp
A,10,2023-01-01T10:00:00Z
B,60,2023-01-01T10:05:00Z
A,30,2023-01-01T10:10:00Z
C,75,2023-01-01T10:15:00Z
B,45,2023-01-01T10:20:00Z
A,90,2023-01-01T10:25:00Z
```

### 3. GitHub Actions Push Workflow (`.github/workflows/ci.yml`)

A GitHub Actions workflow is configured to automate the testing, execution, and deployment steps on every `push` to the `main` branch or a `pull_request`.

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas==2.3 ruff openpyxl

    - name: Convert data.xlsx to data.csv
      # This step ensures data.csv is always present for execute.py
      # in the CI environment, deriving it from the source data.xlsx.
      run: |
        python -c "import pandas as pd; df = pd.read_excel('data.xlsx'); df.to_csv('data.csv', index=False);"

    - name: Run Ruff checks
      run: |
        ruff check .
        ruff format . --check # Ensures formatting adherence

    - name: Execute analysis script and generate result.json
      run: |
        python execute.py > result.json

    - name: Upload artifact for GitHub Pages
      uses: actions/upload-pages-artifact@v3
      with:
        path: 'result.json'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
```

**Workflow Steps Explained:**

*   **Checkout repository**: Fetches the code from the repository.
*   **Set up Python 3.11**: Configures the environment with the specified Python version.
*   **Install dependencies**: Installs `pandas` (version 2.3), `ruff` (for linting), and `openpyxl` (required by pandas to read `.xlsx` files).
*   **Convert `data.xlsx` to `data.csv`**: A Python one-liner to perform the data conversion, ensuring `data.csv` is available for `execute.py`.
*   **Run Ruff checks**: Executes `ruff check .` to lint the Python code and `ruff format . --check` to verify code formatting, providing output in the CI log.
*   **Execute analysis script**: Runs `execute.py`, directing its standard output to `result.json`.
*   **Upload artifact for GitHub Pages**: Takes `result.json` and prepares it as an artifact for GitHub Pages deployment.
*   **Deploy to GitHub Pages**: Publishes the uploaded artifact to the `github-pages` environment. The `result.json` file will be accessible via GitHub Pages at a URL like `https://<YOUR_USERNAME>.github.io/<YOUR_REPO_NAME>/result.json`.

## Local Setup and Usage

To run the script locally and generate `result.json`:

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-name>
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install pandas==2.3 ruff openpyxl
    ```

4.  **Convert `data.xlsx` to `data.csv`:**
    ```bash
    python -c "import pandas as pd; df = pd.read_excel('data.xlsx'); df.to_csv('data.csv', index=False);"
    ```
    *Note: This `data.csv` should be committed to the repository as per the project requirements.*

5.  **Run Ruff checks:**
    ```bash
    ruff check .
    ruff format . --check
    ```

6.  **Execute the analysis script:**
    ```bash
    python execute.py
    # This will create result.json in the current directory
    ```

## Output

The `result.json` file, generated by `execute.py`, will contain the filtered and processed data in JSON format. This file is published directly to GitHub Pages via the CI/CD workflow.

## License

This project is licensed under the MIT License - see the `LICENSE` file for details.
